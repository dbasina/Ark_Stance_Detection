Omni-pretraining stage:
Epoch 0000:
MaskedABSA_politic Validation Loss = 0.65407:
MaskedABSA_politic: 
Student ACCURACY = 0.9, 
Teacher ACCURACY = 0.9
MaskedABSA_politic: 
Student AUC = [0.7778	0.    ], 
Teacher AUC = [0.4444	0.4444]
MaskedABSA_politic: Student mAUC = 0.3889, Teacher mAUC = 0.4444

Omni-pretraining stage:
Epoch 0001:
MaskedABSA_politic Validation Loss = 0.64401:
MaskedABSA_politic: 
Student ACCURACY = 0.9, 
Teacher ACCURACY = 0.9
MaskedABSA_politic: 
Student AUC = [0.7778	0.    ], 
Teacher AUC = [0.5556	0.4444]
MaskedABSA_politic: Student mAUC = 0.3889, Teacher mAUC = 0.5000

Omni-pretraining stage:
Epoch 0002:
MaskedABSA_politic Validation Loss = 0.63102:
MaskedABSA_politic: 
Student ACCURACY = 0.9, 
Teacher ACCURACY = 0.9
MaskedABSA_politic: 
Student AUC = [0.7778	0.    ], 
Teacher AUC = [0.5556	0.3333]
MaskedABSA_politic: Student mAUC = 0.3889, Teacher mAUC = 0.4444

Omni-pretraining stage:
Epoch 0003:
MaskedABSA_politic Validation Loss = 0.61644:
MaskedABSA_politic: 
Student ACCURACY = 0.9, 
Teacher ACCURACY = 0.9
MaskedABSA_politic: 
Student AUC = [0.7778	0.    ], 
Teacher AUC = [0.5556	0.3333]
MaskedABSA_politic: Student mAUC = 0.3889, Teacher mAUC = 0.4444

Omni-pretraining stage:
Epoch 0004:
MaskedABSA_politic Validation Loss = 0.60131:
MaskedABSA_politic: 
Student ACCURACY = 0.9, 
Teacher ACCURACY = 0.9
MaskedABSA_politic: 
Student AUC = [0.7778	0.    ], 
Teacher AUC = [0.5556	0.3333]
MaskedABSA_politic: Student mAUC = 0.3889, Teacher mAUC = 0.4444



Final Results after Omni-pretraining on datasets ['MaskedABSA_politic'] for 5 epochs:
Omni-pretraining stage: 
Student Accuracy = 
[[0.9]
 [0.9]
 [0.9]
 [0.9]
 [0.9]] 
Teacher Accuracy = 
[[0.9]
 [0.9]
 [0.9]
 [0.9]
 [0.9]]
Omni-pretraining stage: 
Student meanAUC = 
[[0.3889]
 [0.3889]
 [0.3889]
 [0.3889]
 [0.3889]] 
Teacher meanAUC = 
[[0.4444]
 [0.5   ]
 [0.4444]
 [0.4444]
 [0.4444]]
Omni-pretraining stage:
Epoch 0000:
MaskedABSA_politic Validation Loss = 0.57382:
MaskedABSA_politic: 
Student ACCURACY = 0.7822, 
Teacher ACCURACY = 0.2178
MaskedABSA_politic: 
Student AUC = [0.4896	0.5409], 
Teacher AUC = [0.6346	0.5178]
MaskedABSA_politic: Student mAUC = 0.5152, Teacher mAUC = 0.5762

Omni-pretraining stage:
Epoch 0001:
MaskedABSA_politic Validation Loss = 0.53314:
MaskedABSA_politic: 
Student ACCURACY = 0.7822, 
Teacher ACCURACY = 0.3168
MaskedABSA_politic: 
Student AUC = [0.4856	0.5535], 
Teacher AUC = [0.6226	0.5155]
MaskedABSA_politic: Student mAUC = 0.5196, Teacher mAUC = 0.5690

Omni-pretraining stage:
Epoch 0002:
MaskedABSA_politic Validation Loss = 0.52668:
MaskedABSA_politic: 
Student ACCURACY = 0.7822, 
Teacher ACCURACY = 0.4455
MaskedABSA_politic: 
Student AUC = [0.4856	0.5575], 
Teacher AUC = [0.6191	0.5196]
MaskedABSA_politic: Student mAUC = 0.5216, Teacher mAUC = 0.5693

Omni-pretraining stage:
Epoch 0003:
MaskedABSA_politic Validation Loss = 0.52852:
MaskedABSA_politic: 
Student ACCURACY = 0.7822, 
Teacher ACCURACY = 0.5941
MaskedABSA_politic: 
Student AUC = [0.4879	0.5644], 
Teacher AUC = [0.6122	0.519 ]
MaskedABSA_politic: Student mAUC = 0.5262, Teacher mAUC = 0.5656

Omni-pretraining stage:
Epoch 0004:
MaskedABSA_politic Validation Loss = 0.53136:
MaskedABSA_politic: 
Student ACCURACY = 0.7822, 
Teacher ACCURACY = 0.6337
MaskedABSA_politic: 
Student AUC = [0.4942	0.5627], 
Teacher AUC = [0.6122	0.5167]
MaskedABSA_politic: Student mAUC = 0.5285, Teacher mAUC = 0.5644



Final Results after Omni-pretraining on datasets ['MaskedABSA_politic'] for 5 epochs:
Omni-pretraining stage: 
Student Accuracy = 
[[0.7822]
 [0.7822]
 [0.7822]
 [0.7822]
 [0.7822]] 
Teacher Accuracy = 
[[0.2178]
 [0.3168]
 [0.4455]
 [0.5941]
 [0.6337]]
Omni-pretraining stage: 
Student meanAUC = 
[[0.5152]
 [0.5196]
 [0.5216]
 [0.5262]
 [0.5285]] 
Teacher meanAUC = 
[[0.5762]
 [0.569 ]
 [0.5693]
 [0.5656]
 [0.5644]]
